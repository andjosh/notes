---
title: "Import AI 314: Language Models + Text-to-Speech; Emergent Cooperation in Wargames; ICML Bans LLM-written Papers (453571777)"
tags: articles-23119910
canonical: https://jack-clark.net/2023/01/09/import-ai-314-language-models-text-to-speech-emergent-cooperation-in-wargames-icml-bans-llm-written-papers/
---

[[_Source_: Import AI 314: Language models + text-to-speech; emergent cooperation in wargames; ICML bans LLM-written papers - [Source URL](https://jack-clark.net/2023/01/09/import-ai-314-language-models-text-to-speech-emergent-cooperation-in-wargames-icml-bans-llm-written-papers/){:target="_blank"}<br>
_Author_: Jack Clark<br>
[Readwise URL](https://readwise.io/open/453571777){:target="_blank"}
::wrap]]

his is an example of the ‘capability overhang’ phenomenon I’ve been talking about re language models for a while – existing LLMs are *far more capable than we think*. All it takes is some experimentation and finding experts to find new ways to phrase questions and you can wind up with extraordinarily powerful capability jumps *without retraining the model*.
