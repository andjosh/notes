---
title: "Use a small model to generate a 'draft' output, then ..."
tags: ai scaling performance articles-24129773
canonical: mailto:reader-forwarded-email/3fed8d021c102ea6eb0fd341ab68ebfd
hide_title: true
---

Use a small model to generate a 'draft' output, then use a larger and smarter model to score the 'draft', then use a rejection sampling scheme to accept the tokens which are agreed by the small and large models.  

In tests, they find that a draft model can give them speedups ranging between 1.92XÂ  (on a summarization benchmark called XSum) and 2.46X on a code generation task called HumanEval.


[[_Source_: Fwd: Import AI 317: DeepMind speeds up language model sampling; voice cloning tech gets abused; more scaling laws for RL _(via email)_<br>
_Author_: Josh Beckman<br>
_More_: [Readwise URL](https://readwise.io/open/471678563){:target="_blank"}
::wrap]]