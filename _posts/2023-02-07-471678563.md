---
title: "Fwd: Import AI 317: DeepMind Speeds Up Language Model Sampling; Voice Cloning Tech Gets Abused; More Scaling Laws for RL (471678563)"
tags: ai scaling performance articles-24129773
canonical: mailto:reader-forwarded-email/3fed8d021c102ea6eb0fd341ab68ebfd
---

[[_Source_: Fwd: Import AI 317: DeepMind speeds up language model sampling; voice cloning tech gets abused; more scaling laws for RL - [Source URL](mailto:reader-forwarded-email/3fed8d021c102ea6eb0fd341ab68ebfd){:target="_blank"}<br>
_Author_: Josh Beckman<br>
[Readwise URL](https://readwise.io/open/471678563){:target="_blank"}
::wrap]]

Use a small model to generate a 'draft' output, then use a larger and smarter model to score the 'draft', then use a rejection sampling scheme to accept the tokens which are agreed by the small and large models.  

In tests, they find that a draft model can give them speedups ranging between 1.92XÂ  (on a summarization benchmark called XSum) and 2.46X on a code generation task called HumanEval.
