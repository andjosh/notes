---
title: "The next problem we have is how to build this ..."
tags: software data search articles-24113148
canonical: https://github.blog/2023-02-06-the-technology-behind-githubs-new-code-search/
author: Timothy Clem
book: 24113148
hide_title: true
---

The next problem we have is how to build this index in a reasonable amount of time (remember, this took months in our first iteration). As is often the case, the trick here is to identify some insight into the specific data we’re working with to guide our approach. In our case it’s two things: Git’s use of [content addressable hashing](https://en.wikipedia.org/wiki/K-way_merge_algorithm) and the fact that there’s actually quite a lot of duplicate content on GitHub. Those two insights lead us the the following decisions:

1.  **Shard by Git blob object ID** which gives us a nice way of evenly distributing documents between the shards while avoiding any duplication. There won’t be any hot servers due to special repositories and we can easily scale the number of shards as necessary.
2.  **Model the index as a tree** and use delta encoding to reduce the amount of crawling and to optimize the metadata in our index. For us, metadata are things like the list of locations where a document appears (which path, branch, and repository) and information about those objects (repository name, owner, visibility, etc.). This data can be quite large for popular content.
[[Embrace the shape of your data/domain, build vs. buy::rmn]]


[[<cite>_[The technology behind GitHub’s new code search](https://github.blog/2023-02-06-the-technology-behind-githubs-new-code-search/){:target="_blank"}_</cite> by Timothy Clem ![favicon](https://s2.googleusercontent.com/s2/favicons?domain=github.blog){:class="source-favicon"}<br>
_More_: [Readwise URL](https://readwise.io/open/471445418){:target="_blank"}
::wrap]]